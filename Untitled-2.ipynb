{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a002dbf",
   "metadata": {},
   "source": [
    "## Uk Weather Persistence Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b65b5",
   "metadata": {},
   "source": [
    "### 22/09:\n",
    "First Meeting with Simon. \n",
    "Aims:\n",
    "- Read 5-10 articles on the subject to improve knowledge and gain a early definition of `Persistence'. Best way to do this is likely by looking at what papers cite Jones et al [(2013)](https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.3498).\n",
    "- Create some plots to examine the data given and gain understanding of potential trends that may be possibly occuring.\n",
    "- Look at 2025 firstly. It's been a fairly dry year, so see what's been happening in relation to the Lamb Weather Types (LWTs), and read Jones et al properly.\n",
    "I am going to use noteable to keep notes for my project and to perform all the coding and data analysis.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef01c7",
   "metadata": {},
   "source": [
    "### 24/09 \n",
    "Took the data and used the pandas package in python to perform all of the data handling. First plot of average daily pressure measurements for the last three years was created, but it doesn't appear to be very useful - it's just purely chaotic, with not really any trend noticeable. We also plot the number of cyclonic days per year before september, for the last 30 years.\n",
    "The data contains daily information about daily surface level pressure measurements, vorticity, direction and magntitude of resultant flow, and the associated LWT of all of this information. The LWTs are numbered 1-28, each number corresponding to a different type. Proceeded to plot the number of cyclonic days per year for the last 30 years, using the matplotlib package in python.\n",
    "\n",
    "\n",
    "<img src=\"cyclonic_days.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a574e82",
   "metadata": {},
   "source": [
    "### 01/10\n",
    "Met with Simon again. Next need to begin writing an very rough introduction - Simon thinks this well help with getting to grips with the subject, and force me to get more references. Created a function that sorts all the LWTs into three different types: Cylonic 'C', Anti-cyclonic 'A', and neither 'N'. Psuedo-code is given below.\n",
    "\n",
    "```\n",
    "def categorize(x):  \n",
    "    if x has an Anti-cyclonic component:\n",
    "        return 'Anti-Cyclonic'\n",
    "    else if x has neither Anti-cyclonic or cyclonic component:\n",
    "        return 'Neither'\n",
    "    else if x has a Cyclonic component:\n",
    "        return 'Cyclonic'\n",
    "    else:\n",
    "        return 'Other'           # This was an error, and picked the unidentifiable data. Later removed entirely, and replaced with 'Neither'\n",
    "\n",
    "df['Grouped_LWT'] = df['LWT'].apply(categorize)\n",
    "```\n",
    "This now groups all the LWT for analysis. We will focus on the Cyclonic types and Anti-cyclonic types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca950ddf",
   "metadata": {},
   "source": [
    "### 02/10\n",
    "Potential definition for Persistence; the conditional likelihood that if LWT type $i$ on day$(t)$, then day$(t+1)$ has LWT type $i$, from [De Luca et al.](https://doi.org/10.3390/atmos10100577.) This could be calculated through the data fairly simply, by potentially seeing if the next entry is the same as the current entry and returning a true value if so, then repeating in a for loop for all entries in the dataset and saving the result in another dataset. You could do this for every year and calculate the percentage of values that are true, which would give a measure of persistence; and see if that has gone up or down since 1871. Could also be done for each LWT individually, and seeing how persistence each is, instead of doing it for all in one. The study above (De Luca) used markov chain transition matrices to predict future behaviour. This could be a useful thing to examine to interpret the data for many all As well, could sort data into different months based of season: DJF, MAM, JJA, SON. Found another defintion of persistence as well from Pfleiderer et al [(2017)](https://doi.org/10.1007/s00382-017-3945-x), based off of the average length of hot and cold days, were hot and cold are defined by comparison against the average temperature of a season.\n",
    "\n",
    "Created the function for De Luca's persistence:\n",
    "```\n",
    "def persistence_count(df,types):\n",
    "    ''' calculates the conditional probability that day(t+1) had grouped weather type i, given day(t) had grouped weather type i, for types given.'''\n",
    "    numer of days where day(t) has same type as day (t+1) = count_1 =  0\n",
    "    number of days with that type = count_2 = 0\n",
    "    for v in types:\n",
    "        for i in range(length of datset - 1):\n",
    "            if i-th entry of the groupedLWT == v and (i+1)-th entry of the groupedLWT == v:\n",
    "                count_1 += 1\n",
    "            if i-th entry of the GroupedLWT == v:\n",
    "                count_2 += 1\n",
    "    return count_1/count_2 if count_2 > 0 else 0 \n",
    "\n",
    "```\n",
    "This returns persistence values for as many 'Types' as inputted, where the types are the different grouped LWTs under consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e40e30",
   "metadata": {},
   "source": [
    "### 04/10\n",
    "Got more metrics that could quantify persistence: \n",
    "- Average length of a 'period', where a period is just a block of consecutive weather type $i$.\n",
    "- Average time between periods.\n",
    "Developed functions to find all of these, and started writing the introduction.\n",
    "\n",
    "\n",
    "Potential future analyses approach:\n",
    "- Calculate persistence for all sorted LWTs, for all seasons to get one value per year of persistence per year, and plot. Could calculate average persistence every 10 years, and calculate variance(Why?). Calculate persistence again for all seasons individually.\n",
    "- Could Calculate monthly persistence, and find a mean and variance for it per season per year/decade? Would return 12 values per year. May be easier in R.\n",
    "\n",
    "Started writing the introduction, but finding it challenging. Need to really expand on what I've said, and I think having Lamb's original 1971 article could be really beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159adad",
   "metadata": {},
   "source": [
    "### 08/10\n",
    "Met with Simon again - he recommends spending maybe 2 or 3 days just purely writing, as my introduction isn't very long and doesn't go into enough depth. He also approved of splitting the data up, but not into seasons of three months, instead just splitting it into 'hot' and 'cold' seasons. For any plots he recommended calculating the average persistence for 10 year period, due to there being a lot of variability in the data on a year to year basis. Created functions for average length of a period, and average time between periods. Plotted the persistence per year, using the persistence count function. \n",
    "\n",
    "<img src=\"PersistenceHot.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ce8d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4490d620",
   "metadata": {},
   "source": [
    " ## 12/10:\n",
    "- continue with more analyses on the data; \n",
    "- potentially investigate the use of poisson point processes, homogenous, to model how storms arrive, and then we can estimate the intensity paramater to see how it has changed over time, and even create confidence intervals instead of going through the uncertainty route. Model shouldn't be too much of an approximation. Have a look at the Stochastic modelling notes to see if we could use a Markov Chain for the De Luca persistence measure. \n",
    "- Validate that storms actually line up with cyclonic days.\n",
    "- examine Jones et al to see if they have any comments on the quality of data pre 1900 - seems to spike before then, curiously.\n",
    "- Write more up; just methodology for now, not results. Talk about our different measures of persistence, e.g. De Luca's, Average length between storms, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3719b2",
   "metadata": {},
   "source": [
    "### 17/10\n",
    "- continue with more analyses on the data - create a function to find the log likelihood. (not hard)\n",
    "- Had a look at my Stochastic modelling notes - don't think modelling our system as a markov chain would be worthwhile, as all conclusion we can draw from it mainly look at the steady state, and there is too much variability for us to reach the steady state.\n",
    "- Point process seems to be worthwhile. Issues arise with the discrete time approximation, and the clustering of storms. This can be surpassed, as there are enough days to make the approximation accurate enough and the clustering can be by only looking at only when the storms arrive, and not looking at how long they last. This process is called declustering, https://doi.org/10.1002/joc.3458 has a good article about this, as well as https://doi.org/10.1016/j.wace.2013.07.003. Seems a valid approach.\n",
    "- need to properly have a look and validate that storms do line up. Don't know how as of this moment, but may have to do so manually.\n",
    "- Jones was examined, and no real criticisms of the data were given. Quality of data 'generally good' for mid latitudes.\n",
    "- Done a fair bit more writing, though now need to write up details about the log likelihood function and MLEs, and confidence intervals.\n",
    "- Given our De Luca function just finds the MLE for the paramater for a binomial distribution, we can also find confidence intervals for that. Look at statistical methodology notes, but otherwise these portions are looking promising. This isn't much of an approximation as well. \n",
    "- Found a paper specifically discussing persistence in weather, https://doi.org/10.5194/esd-14-955-2023. They differentiate between two types: Quasi-stationary persistence, related to how long conditions stay similar which is the category De Luca's falls under; and Recurrent persistence, which is related to gaps between storms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9477ac5",
   "metadata": {},
   "source": [
    "### 19/10 \n",
    "\n",
    "Created code for both MLE, for Quasi-stationary and Recurrent.\n",
    "```\n",
    "def MLE_QS(df,types):\n",
    "    '''Returns 95% confidence interval for the QS MLE.'''\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    for v in types:\n",
    "        for i in range(len(df['Grouped_LWT']) - 1):\n",
    "            if df['Grouped_LWT'].iloc[i] == v and df['Grouped_LWT'].iloc[i + 1] == v:\n",
    "                count_1 += 1\n",
    "            if df['Grouped_LWT'].iloc[i]==v:\n",
    "                count_2 += 1\n",
    "    if count_2 > 0:\n",
    "        MLE = count_1/count_2     #            # Probability calculation\n",
    "        SE =((MLE*(1-MLE))/count_2)**1/2\n",
    "        zSE = 1.96*SE     # Standard error calculation\n",
    "        lower = MLE - zSE\n",
    "        upper = MLE + zSE\n",
    "        lower = max(0, lower)         # Ensuring logical bounds for our CI\n",
    "        upper = min(1, upper)\n",
    "\n",
    "        return [MLE,SE,(lower,upper),count_2]\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "```\n",
    "```\n",
    "def MLE_R(df,types):\n",
    "    '''Returns the MLE,SE,CI, for a data frame that starts on day a and finishes on day t.'''\n",
    "    Exposure = the time over which the MLE is calculated for, t-a\n",
    "    for v in types:\n",
    "        number of periods = 0\n",
    "        current period length = 0\n",
    "        for i in range(length of dataset):\n",
    "            if i-th position == v:\n",
    "                current period length += 1\n",
    "                if i == last entry:\n",
    "                    number of periods += 1\n",
    "            else:\n",
    "                # close out a run if one was active\n",
    "                if current_len > 0:\n",
    "                    num_periods += 1               \n",
    "                    current_len = 0\n",
    "    MLE = num_periods/t_end       # MLE calculation\n",
    "    SE = (MLE/t_end)**1/2\n",
    "    zSE = 1.96*SE        # Standard Error calculation\n",
    "    confidence interval = (MLE-zSE,MLE+zSE)\n",
    "        \n",
    "    return [MLE, SE, Confidence Interval, exposure]\n",
    "```\n",
    "\n",
    "These return the MLE, Standard errors, confidence intervals for both. For the recurrent MLE, it also returns the exposure. For the QS MLE, it also return total number of days that had the type. \n",
    "\n",
    "These function are essential carrying out these equations, for the Recurrent persistence rate and the Quasi-stationary rate respectively\n",
    "\n",
    "$\\hat\\lambda = \\frac{\\text{ number of arrivals}}{\\text{exposure}}$\n",
    "\n",
    "$\\hat{P_{ii}} = \\frac\n",
    "\n",
    "\n",
    "Plots were created based on the MLE estimate for every ten years (pseudo code below), and plotted. Very small errors were observed, which is curious given how chaotic the data appeared to be on first examination.\n",
    "\n",
    "```\n",
    "MLEs = []\n",
    "for i in range(15):\n",
    "       df_iCold = Cold dataset between 1875+10*i and 1875 +(10+1)*i\n",
    "       df_iHot = Hot dataset between 1875+10*i and 1875 +(10+1)*i\n",
    "       MLEs.append(Maximum Likelihood Estimate for hot and cold,\n",
    "                  Anti-cyclonic and cyclonic, \n",
    "                  Recurrent and Quasi-stationary)\n",
    "       \n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae6ab8",
   "metadata": {},
   "source": [
    "### 19/10\n",
    "- Compare named storms to data still.\n",
    "- Took Lamb's definitions from his [1972](https://digital.nmla.metoffice.gov.uk/IO_b5b72a52-53e7-42a0-b21e-7e0e314bd42b/ ) paper for what defines an anti-cyclone and a cyclone\n",
    "- Wrote about the binomial distribution, somewhat briefly - could maybe go into further depth.\n",
    "- Created functions to find the two MLEs, that find: MLE, Standard Error, Confidence Interval, and total storm days/total days.\n",
    "- Standard error seems extremely low for both cases; somewhat curiously given the amount of apparent chaos and variation, so looked again and discovered and error. The code was fixed to show accurate standard errors. In both functions, the equation to find the standard error was incorrect. It took the power of ```^1/2```, which actually raises it to the power of 1 then divides by two. It was changed to ```^0.5```, which accurately finds the square root. Errors become substantialy larger, and the plots do not show any particularly obvious trends. These two plots are for grouped LWT cyclonic persistence in winter.\n",
    "\n",
    "\n",
    "<img src=\"MLE_Quasi-stationary_per_decade_ColdCyclonic.png\" width=\"300\">  <img src=\"MLE_recurrent_per_decade_ColdCyclonic.png\" width=\"300\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9900bc",
   "metadata": {},
   "source": [
    "### 22/10\n",
    "Met with Simon. He recommended potentially looking at covariate fits for the data with potentially using some metric of temperature change as a predictor. Simon also emailed Ed Hawkinds at University of Reading with a query over 20CRv3 reliability. Ed said that the quality of data for 20CRv3 is generally good, though care should be taken around WW1, as there are more ship observations around that time that skew results by under reading the pressure. Also, there are not enough observations to reliably represent intense windstorms. This should be mentioned as a limitation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d09a0",
   "metadata": {},
   "source": [
    "### 25/10\n",
    "Extended De Luca's definiton in order to give the 3x3 transition matrix, so that the reader can better understand average trends that occur. Although when it was found, there appeared to be an error in that not all of the rows added up to equal 1, which must be the case for a transition matrix. Discovered that an error had been made in the constructed categorize function from earlier, as it categorized the LWT into four categories: AC,C,N, and 'other' which is mainly the U type which corresponds to not identified weather type days. Fixed it, such that this went into N, and the rows of the matrix then added to one.\n",
    "\n",
    "\\begin{pmatrix}\n",
    "0.63 & 0.06 & 0.31 \\\\\n",
    "0.11 & 0.55 & 0.34 \\\\\n",
    "0.25 & 0.24 & 0.52\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "Any errors in addition are due to rounding errors here. We can see that each regime is most likely to transition to itself, and there rarely occurs a transition from cylonic to anticyclonic\n",
    "Also, went back five years and manually put every named storm, and their dates into a dataset, and it turns out that only 57% of them actually have cyclonic behaviour according to our dataset. This is lower than we would expect it to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206fe3e7",
   "metadata": {},
   "source": [
    "###  26/10\n",
    "Changed to VScode, as kept having bugs in noteable.\n",
    "- To examine whether or not our MLE changes, we can let $\\log[\\frac{\\hat{P}_{ii}^{(t)}}{1-\\hat{P}_{ii}^{(t)}}] = \\beta_0 +\\beta_1t+ \\epsilon_t$. Can choose a certain timeframe to look at this over. (binomial regression) https://timeseriesreasoning.com/contents/binomial-regression-model/.\n",
    " but may be an interesting approach. For recurrence, we may use poisson regression, which is similar but just use $\\log$. These are both examples of generalized linear models, and we can perform hypothesis tests over whether or not they have a slope of 0, so that we can obtain statistically significant answers.\n",
    "- De Luca found the NCEP and 20CR disagree about persistence of Anticyclonic types in summer. (NCEP $\\approx$ 0.5, 20CR $\\approx$ 0.65), over the 80s\n",
    "-  ``(i) 20CR overestimates cloud fraction and precipitation [https://www.tandfonline.com/doi/abs/10.1080/01621459.1968.10480934]; and (ii) NCEP underestimates the temperature, overestimates the wind-speed and monthly precipitation variability [https://doi.org/10.1175/JCLI-D-11-00004.1].\" This is particularly relevant, as our data from Jones et al is composed of this.\n",
    "- Turns out than when examined for 20 year period, more interesting data for summer anticyclonic MLE shows; Recurrrence shows patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73935b",
   "metadata": {},
   "source": [
    "### 29/10 \n",
    "Did more research into Generalized linear models look promising, and I created a functions that takes the Calculated MLEs from a dataframe, and finds the logit/logarithm of them in a new column. For crafting the GlM, we use Scikit.learn and its ```linear_regression``` function to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07894602",
   "metadata": {},
   "source": [
    "### 09/11\n",
    "\n",
    "Took the [HADCRUT5](https://doi.org/10.1029/2019JD032361) dataset and imported it to python using pandas. The version I sourced is composed of the yearly global mean temperature anomaly. I took this yearly data, and found the rolling average for every 10 years, for future use with GLMs as a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0eae9",
   "metadata": {},
   "source": [
    "### 11/11\n",
    "After meeting with simon, have decided to add more plots for the MLEs so we could have behaviour for both MLEs over both seasons analysed for anti-cyclonic and cyclonic, and changed the time-scale to be 30 years.\n",
    "\n",
    "<img src=\"4_plots_quasi.png\" width=\"300\">  <img src=\"4_plots_recurrent.png\" width=\"300\">\n",
    "\n",
    "It looks like there is a negative trend in time somewhat for some of the quasi-stationary persistence MLEs, and an increase in time for the recurrent MLE for Anti-cyclones in the hot season. This is interesting, and could be worth looking at the literature about atmospheric blocking,\n",
    "\n",
    "For the generalized linear models, we will take the MLE and calculate their logarithm for recurrent, and the logit for quasi-stationary, then fit these to a linear regression model using Scikit learn regression, using a time/temperature as our predictor. Then we will perform a hypothesis test that the slope is equal to 0, and find the p-values for such a test.\n",
    "\n",
    "\n",
    "```\n",
    "def regression_diagnostics(model, X, y, alpha=0.05):\n",
    "      takes predictor X, observables y, the model, and the confidence interval alpha \n",
    "      and uses the Scikit.learn linear_regression() function and returns the slope estimate, slope standard error, and the p-value of the hypothesis test that the slope is equal 0.\n",
    "```\n",
    "\n",
    "The p Values after all these models have been ran are given below. \n",
    "\n",
    "\n",
    "Temperature as a predictor:            \n",
    "\n",
    "<img src=\"df_p_temp.png\" width=\"300\"> , \n",
    "   \n",
    "Time as a predictor:\n",
    "\n",
    "<img src=\"df_p_time.png\" width=\"300\"> \n",
    "\n",
    "Hence we can see that there is evidence of a relationship between Anti-cyclonic recurrent persistence in the hot season, and we know it to be a positive relationship. We gain similar results when we use Temperature. In fact when we tested how 'similar' these predictors were, we obtained that time and temperature had a correlation coefficient of 0.9, indicating strong correlation, indicating that it is likely not appropriate to use both as an predictor of persistence cjhange in this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19586f18",
   "metadata": {},
   "source": [
    "### 13/11 09:52\n",
    "\n",
    "Chnaged the data to use 10 year increments over 30 years, as the model handles the error itself so we do not need to it ourselves. Created plots for this and looked at the slopes. The value of the slope parameter was very small, indicating only a slight change in the parameter every 10 years, however as it is multiplicative change due to the nature of GLMs, we see it compounds easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc01cb",
   "metadata": {},
   "source": [
    "### 16/11\n",
    "\n",
    "Realised that the approach was largely incorrect for using the MLE as our observation, and instead the GLM called for the counts per period, and not a value of MLE. Also, the variance for each observation will vary, which is not allowed for a linear regression model which requires homoscedasticity (equal variance), due to the differing number of trials for the Quasi-stationary binomial regression and the differing exposure for the Recurrent Poisson regression, which scikit.learn's ```linear_regression()``` does not take into account. Hence, we move onto to using [StatsModels.api](https://www.statsmodels.org/stable/glm.html) ```glm()``` function, which allows us to encode which distribution our parameters have, what 'link' function to use, and what the exposure is. \n",
    "\n",
    "```\n",
    "def P_value_table_qs(years,successes,trials,alpha):\n",
    "       inputs  all the successes and trials for multiple models, and confidence level alpha and \n",
    "       returns a pandas dataframe with the slope, slope standard error, p-value for slope = 0 hypothesis tests \n",
    "       and McFadden's R^2 for all of them, using sm.glm(), with family being specified as binomial.\n",
    "    \n",
    "\n",
    "```\n",
    "```\n",
    "def P_value_table_R(years,number_of_arrivals,exposure,alpha):\n",
    "      inputs the number of arrivals and the exposure for multiple models and confidence level alpha and \n",
    "       returns a pandas dataframe with the slope, slope standard error, p-value for slope = 0 hypothesis tests \n",
    "       and McFadden's R^2 for all of them, using sm.glm(), with family being specified as Poisson.\n",
    "```\n",
    "\n",
    "In order to obtain number of arrivals and exposure for $\\lambda$, and the number of sucesses and trials for $P_{ii}$, we modify our original MLE functions to output this (as they already count these), and input them into the glm() function to obtain our models. This function has a built in p-value attribute obtained from ```.pvalue```, allowing us to automatically take p-values from the model, and take them for our slope parameters. These p-values are generated from the Wald test statistic, and are tested with the null hypothesis that the slope is equal to $0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784166c",
   "metadata": {},
   "source": [
    "### 19/11\n",
    "\n",
    "Plotted these generalized linear models for the models with p-value less than 0.05, i.e. statistically significant at the $95$\\% confidence interval, and these plots are given below. \n",
    "\n",
    "<img src=\"Poisson regression(true).png\" width=\"300\">      <img src=\"Binomial regression_true.ACcold.png\" width=\"300\">        <img src=\"Binomial regression_true.CHot.png\" width=\"300\">   \n",
    "\n",
    "rightmost two are our binomial regression results, left is the Poisson regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4889ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
